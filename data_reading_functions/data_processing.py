# -*- coding: utf-8 -*-
"""Data_processing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GWGHzpPLY_hK2AkBugkrFONZ1i2PwFnj
"""

# run this cell only once if the rest of the code is not working (for example if wntr is not found)
!pip install numpy==1.23.5
!pip install wntr==1.2
!apt-get install libsuitesparse-dev && pip install scikit-sparse

import sys
import os
import numpy as np
from numpy import linalg as la
import networkx as nx
import pandas as pd
import wntr
import matplotlib.pyplot as plt
import copy
from datetime import datetime, timedelta
import pdb
import scipy.sparse as sp
import warnings
warnings.filterwarnings('ignore')

# improve matplotlib image quality
plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
import matplotlib_inline
matplotlib_inline.backend_inline.set_matplotlib_formats('svg')

if 'google.colab' in sys.modules:
  # Check if the drive has been mounted
  drive_mounted = os.path.ismount('/content/drive')
  if drive_mounted:
    print("Google Drive is mounted.")
  else:
    from google.colab import drive
    drive.mount('/content/drive')

  net_dir = '/content/drive/MyDrive/FYP Code/'
else:
  # replace with local directory
  pass

net_name = 'BWFLnet.inp'

# load network data
# wdn = load_network_data(os.path.join(net_dir, net_name))

def extract_reservoir_patterns(inp_file_path):

  """
  For some reason we were not able to extract the pattern name for the reservoirs
  using the node.head_pattern_name method. Therefore we are manually parsing through
  the .inp file and extracting the pattern name for each reservoir.
  """
  res_pattern_map = {}
  with open(inp_file_path, 'r') as f:
      lines = f.readlines()

  in_res_section = False
  for line in lines:
      stripped = line.strip()
      if stripped.startswith('[RESERVOIRS]'):
          in_res_section = True
          continue
      if in_res_section:
          if stripped.startswith('['):  # next section
              break
          if stripped and not stripped.startswith(';'):
              parts = stripped.split()
              if len(parts) >= 3:
                  res_pattern_map[parts[0]] = parts[2]
              else:
                  res_pattern_map[parts[0]] = None
  return res_pattern_map

def extract_patterns_map(inp_file_path):
    patternsMap = {}
    with open(inp_file_path, 'r') as f:
        lines = f.readlines()

    in_pattern_section = False
    for line in lines:
        line = line.strip()

        if line.startswith('[PATTERNS]'):
            in_pattern_section = True
            continue
        if in_pattern_section and line.startswith('['):
            break  # new section starts, stop processing

        if in_pattern_section and line and not line.startswith(';'):
            parts = line.split()
            pattern_name = parts[0]
            multipliers = list(map(float, parts[1:]))

            if pattern_name not in patternsMap:
                patternsMap[pattern_name] = multipliers
            else:
                patternsMap[pattern_name].extend(multipliers)

    return patternsMap

def makeDemands(junctions_df, patternsMap, timeofDay):
    num_junctions = len(junctions_df)

    # Initialize arrays
    multipliers = np.ones((num_junctions, 1))
    demands = np.ones((num_junctions, 1))
    baseDemands = junctions_df['base_demand'].values.reshape(-1, 1)

    for jj in range(num_junctions):
        base = baseDemands[jj][0]
        pattern = junctions_df.loc[jj, 'pattern']

        if pd.isna(pattern) or pattern == 'None':
            multipliers[jj] = 1
            demands[jj] = base
        else:
            if pattern in patternsMap:
                patt_vec = patternsMap[pattern]
                if timeofDay < len(patt_vec):
                    multipliers[jj] = patt_vec[timeofDay]
                    demands[jj] = base * multipliers[jj]
                else:
                    multipliers[jj] = 1
                    demands[jj] = base
            else:
                multipliers[jj] = 1
                demands[jj] = base

    return demands, multipliers, baseDemands

def makeFixedHeads(fixedheads, patternsMap, timeofday):
    n = len(fixedheads)
    multipliers = np.ones(n)
    H0 = np.ones(n)
    baseH0 = np.ones(n)

    for jj in range(n):
        pattern = fixedheads.loc[jj, 'pattern']
        baseH0[jj] = fixedheads.loc[jj, 'elev']  # assuming 'elev' is equivalent to 'Head'

        if pattern is None or pattern == '':
            multipliers[jj] = 1
            H0[jj] = baseH0[jj]
        else:
            if pattern in patternsMap:
                patt_vec = patternsMap[pattern]
                if timeofday < len(patt_vec):
                    multipliers[jj] = patt_vec[timeofday]
                    H0[jj] = baseH0[jj] * multipliers[jj]
                else:
                    multipliers[jj] = 1
                    H0[jj] = baseH0[jj]
            else:
                multipliers[jj] = 1
                H0[jj] = baseH0[jj]

    return H0, multipliers, baseH0

wn = wntr.network.WaterNetworkModel(os.path.join(net_dir, net_name))
sim = wntr.sim.EpanetSimulator(wn)
results = sim.run_sim()



nt = int(wn.options.time.duration / wn.options.time.report_timestep)
nt = nt if nt>0 else 1

net_info = dict(
        np=wn.num_links,
        nn=wn.num_junctions,
        n0=wn.num_reservoirs,
        nt=nt,
        headloss=wn.options.hydraulic.headloss,
        units=wn.options.hydraulic.inpfile_units,
        reservoir_names=wn.reservoir_name_list,
        junction_names=wn.junction_name_list,
        pipe_names=wn.pipe_name_list,
        valve_names=wn.valve_name_list,
        prv_names=wn.prv_name_list,
        bv_names = wn.psv_name_list
    )

## extract link data
if net_info['headloss'] == 'H-W':
  n_exp = 1.852
elif net_info['headloss'] == 'D-W':
  n_exp = 2

link_df = pd.DataFrame(
    index=pd.RangeIndex(net_info['np']),
    columns=['link_ID', 'link_type', 'diameter', 'length', 'n_exp', 'C', 'node_out', 'node_in', 'valve_type', 'status'],
    ) # NB: 'C' denotes roughness or HW coefficient for pipes and local (minor) loss coefficient for valves

def link_dict(link):
  if isinstance(link, wntr.network.Pipe): # check if link is a pipe
    return dict(
        link_ID=link.name,
        link_type='pipe',
        diameter=link.diameter,
        length=link.length,
        n_exp=n_exp,
        C=link.roughness,
        node_out = link.start_node_name,
        node_in = link.end_node_name,
        status = str(link.status).split('.')[-1]
    )
  elif isinstance(link, wntr.network.Valve): # check if link is a valve
    return dict(
        link_ID=link.name,
        link_type='valve',
        diameter=link.diameter,
        length=2*link.diameter,
        n_exp=2,
        C=link.minor_loss,
        node_out = link.start_node_name,
        node_in = link.end_node_name,
        valve_type=link.valve_type,
        status = str(link.status).split('.')[-1]
    )

for idx, link in enumerate(wn.links()):
  link_df.loc[idx] = link_dict(link[1])

# extract node data

node_df = pd.DataFrame(
    index=pd.RangeIndex(net_info['nn']+net_info['n0']),
    columns=['node_ID','elev', 'xcoord', 'ycoord', 'base_demand', 'pattern', 'node_type']
)

res_patterns = extract_reservoir_patterns(os.path.join(net_dir, net_name))

def node_dict(node):
  if isinstance(node, wntr.network.Reservoir):
    elev = node.base_head # is this correct or should we set to 0 as done in tutorial file
    base_demand = np.nan
    pattern = res_patterns.get(node.name, None)
    node_type = 'reservoir'
  elif isinstance(node, wntr.network.Tank):
    elev = node.elevation
    base_demand = np.nan  # Tanks donâ€™t have a demand
    pattern = None  # No pattern typically associated with tanks
    node_type = 'tank'
  else:
    elev = node.elevation
    base_demand = node.base_demand
    node_type = 'junction'
    try:
      demand_entry = node.demand_timeseries_list[0]
      pattern = demand_entry.pattern.name if demand_entry.pattern is not None else None
    except (IndexError, AttributeError):
      pattern = None  # No demand or malformed entry
  return dict(
      node_ID=node.name,
      elev=elev,
      xcoord=node.coordinates[0],
      ycoord=node.coordinates[1],
      base_demand = base_demand,
      pattern = pattern,
      node_type = node_type

  )

for idx, node in enumerate(wn.nodes()):
  node_df.loc[idx] = node_dict(node[1])


## compute graph data
A = np.zeros((net_info['np'], net_info['nn']+net_info['n0']), dtype=int)
for k, row in link_df.iterrows():
  # find start node
  out_name = row['node_out']
  out_idx = node_df[node_df['node_ID']==out_name].index[0]
  # find end node
  in_name = row['node_in']
  in_idx = node_df[node_df['node_ID']==in_name].index[0]
  A[k, out_idx] = -1
  A[k, in_idx] = 1

junction_idx = node_df.index[node_df['node_ID'].isin(net_info['junction_names'])].tolist()
reservoir_idx = node_df.index[node_df['node_ID'].isin(net_info['reservoir_names'])].tolist()

A12 = A[:, junction_idx]; A12 = sp.csr_matrix(A12) # link-junction incident matrix
A10 = A[:, reservoir_idx]; A10 = sp.csr_matrix(A10) # link-reservoir indicent matrix

"""
demand_df = results.node['demand'].T
col_names = [f'demands_{t}' for t in range(1, len(demand_df.columns)+1)]
demand_df.columns = col_names
demand_df.reset_index(drop=False, inplace=True)
demand_df = demand_df.rename(columns={'name': 'node_ID'})
"""

print(link_df)

patternsMap = extract_patterns_map(os.path.join(net_dir, net_name))
nl = max(1, max(len(v) for v in patternsMap.values()))

#### JUNCTIONS ####

# Making demands for each loading scenario
demands = np.ones((net_info['nn'], nl))
MultDemands = np.ones((net_info['nn'], nl))
baseDemands = np.ones((net_info['nn'], 1))

for t in range(nl):
    d, m, base = makeDemands(node_df.loc[junction_idx], patternsMap, t)
    demands[:, t] = d.flatten()
    MultDemands[:, t] = m.flatten()
    baseDemands = base  # baseDemands is always (n, 1), okay to leave as-i
# demands = demands/1000
# baseDemands = baseDemands/1000
# values are not matching with matlab script once we divide by 1000


#### RESERVIORS AND TANKS #####

# Combine reservoirs and tanks into one group
fixedhead_df = node_df[node_df['node_type'].isin(['reservoir', 'tank'])].reset_index(drop=True)

num_reservoirs = len(fixedhead_df)
H0 = np.ones((num_reservoirs, nl))
MultH0 = np.ones((num_reservoirs, nl))
baseH0 = np.ones((num_reservoirs, 1))

# Fill values using a makeFixedHeads-like function
for t in range(nl):
    H0[:, t], MultH0[:, t], baseH0 = makeFixedHeads(fixedhead_df, patternsMap, t)


##### Why do we do this ######

# Adjust arrays if nl == 97
if nl == 97:
    demands = demands[:, :96]
    H0 = H0[:, :96]
    MultDemands = MultDemands[:, :96]
    MultH0 = MultH0[:, :96]
    nl = 96

time_info = vars(wn.options.time)
hydraulics_info = vars(wn.options.hydraulic)

# build model

from pydantic import BaseModel
from typing import Any

class WDN(BaseModel):
    A12: Any
    A10: Any
    net_info: dict
    link_df: pd.DataFrame
    node_df: pd.DataFrame
    demands: np.ndarray
    MultDemands: np.ndarray
    baseDemands: np.ndarray
    H0: np.ndarray
    MultH0: np.ndarray
    baseH0: np.ndarray
    time_info: dict
    hydraulics_info: dict

    class Config:
        arbitrary_types_allowed = True

wdn = object()
wdn = WDN(
    A12=A12,
    A10=A10,
    net_info=net_info,
    link_df=link_df,
    node_df=node_df,
    demands=demands,
    MultDemands= MultDemands,
    baseDemands= baseDemands,
    H0= H0,
    MultH0= MultH0,
    baseH0= baseH0,
    PRVs= PRVs,
    BVs= BVs,
    time_info = time_info,
    hydraulics_info = hydraulics_info
    )